{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this for the HK2 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./2.Re_do_with_lambda_macs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the pbs parameters for macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the parameters\n",
    "nodes_ppn = [1, 1]\n",
    "memory = 16\n",
    "wait_time = [20, 00]\n",
    "quene = \"medium\"\n",
    "inpath = '/mount/weili3/xc3/ENL2_ChIP/data/'\n",
    "exps = ['293*', 'HK2*']\n",
    "ctrl = 'input*'\n",
    "treat = 'F*'\n",
    "outpath = '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/'\n",
    "exp = exps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the batches of bash jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_ENL.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_ENL.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T1.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T1.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T2.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T2.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T3.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T3.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_Y78A.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_Y78A.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_ENL.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_ENL.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T1.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T1.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T2.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T2.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T3.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T3.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_Y78A.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_Y78A.nsp_treat_afterfiting_all.wig.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls /mount/weili3/xc3/ENL2_ChIP/res_nsp/*.wig.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mount/weili3/xc3/ENL2_ChIP/data/293_F_T1/293_F_T1.bowtie.bed.nsp.shuf', '/mount/weili3/xc3/ENL2_ChIP/data/293_F_T2/293_F_T2.bowtie.bed.nsp.shuf', '/mount/weili3/xc3/ENL2_ChIP/data/293_F_T3/293_F_T3.bowtie.bed.nsp.shuf', '/mount/weili3/xc3/ENL2_ChIP/data/293_F_Y78A/293_F_Y78A.bowtie.bed.nsp.shuf', '/mount/weili3/xc3/ENL2_ChIP/data/293_F_ENL/293_F_ENL.bowtie.bed.nsp.shuf']\n",
      "/mount/weili3/xc3/ENL2_ChIP/data/293_input_S32/293_input_S32.bowtie.bed.nsp.shuf\n"
     ]
    }
   ],
   "source": [
    "treat_files = glob.glob(inpath+exp+treat+'/*.bowtie.bed.nsp.shuf')\n",
    "ctrl_file = glob.glob(inpath+exp+ctrl+'/*.bowtie.bed.nsp.shuf')[0]\n",
    "print(treat_files)\n",
    "print(ctrl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the initialization header for pbs and read the contents\n",
    "with open('/home/xc3/experiment/initial.pbs','r') as f:\n",
    "\tpbs_header = f.readlines()\n",
    "\n",
    "for exp in exps:\n",
    "\t\n",
    "\ttreat_files = glob.glob(inpath+exp+treat+'/*.bowtie.bed.nsp.shuf')\n",
    "\tctrl_file = glob.glob(inpath+exp+ctrl+'/*.bowtie.bed.nsp.shuf')[0]\n",
    "\tnames = [ifile.split('/')[-2] for ifile in treat_files]\n",
    "\n",
    "\tfor i, treat_file in enumerate(treat_files):\n",
    "\t\texperiment_name = \"macs_{}.ato\".format(names[i])\n",
    "\t\twith open(experiment_name,'w') as f:\n",
    "\n",
    "\t\t\tpbs_initial = pbs_header[:]\n",
    "\t\t\t# configuration for the experiments\n",
    "\t\t\t# pbs_initial[1]::job name\n",
    "\t\t\tpbs_initial[1] = pbs_initial[1].format(str(names[i]))\n",
    "\t\t\t# pbs_initial[2]::nodes and ppn\n",
    "\t\t\tpbs_initial[2] = pbs_initial[2].format(*nodes_ppn)\n",
    "\t\t\t# pbs_initial[4]::memeroy\n",
    "\t\t\tpbs_initial[4] = pbs_initial[4].format(memory)\n",
    "\t\t\t# pbs_initial[5]::waiting time\n",
    "\t\t\tpbs_initial[5] = pbs_initial[5].format(*wait_time)\n",
    "\t\t\t# pbs_initial[11]::err output\n",
    "\t\t\tpbs_initial[11] = pbs_initial[11].format(names[i])\n",
    "\t\t\t# pbs_initial[12]::log\n",
    "\t\t\tpbs_initial[12] = pbs_initial[12].format(names[i])\n",
    "\t\t\t# pbs_initial[14]::quene\n",
    "\t\t\tpbs_initial[14] = pbs_initial[14].format(quene)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t# write all the configurations into the pbs file\n",
    "\t\t\tfor line in pbs_initial:\n",
    "\t\t\t\tf.write(line)\n",
    "\n",
    "\t\t\t# write the shuf code\n",
    "\t\t\tline =\"macs14 -t {} -c {} -n {}.nsp --nomodel -g hs --wig -S -p 1e-8\".format(treat_files[i], ctrl_file, outpath + names[i])\n",
    "\t\t\t\n",
    "\t\t\tf.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotated the above peaks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annoate_bed_to_gene(infile, outfile, cutoff=5000):\n",
    "    ref, tss_set = {}, set()\n",
    "    reffile = '/mount/weili3/xc3/genomes/hg19.refGene.txt'\n",
    "    for line in open(reffile):\n",
    "        col = line.split('\\t')\n",
    "        name, cr, strand, TSS, TES, symbol = col[1], col[2], col[3], int(col[4]), int(col[5]), col[12]\n",
    "        if strand == '-': TSS, TES = TES, TSS\n",
    "        if cr not in ref: ref[cr] = []\n",
    "        if (cr,TSS,strand) not in tss_set:\n",
    "            ref[cr].append((name,symbol,strand,TSS,TES))\n",
    "            tss_set.add((cr,TSS,strand))\n",
    "    for cr in ref: ref[cr].append(('none','none','none',0,0))\n",
    "    \n",
    "    # annotate the file\n",
    "    text = open(infile).readlines()\n",
    "    fout = open(outfile, 'w')\n",
    "    print('processing on {}\\n will output {}\\n'.format(infile,outfile))\n",
    "    for line in text:\n",
    "        col = line.split('\\t')\n",
    "        try: cr, start, end = col[0], int(col[1]), int(col[2])\n",
    "        except: \n",
    "            fout.write(line[:-1]+'\\twithin_genebody\\tnearest_TSS\\tdistance\\n')\n",
    "            continue\n",
    "        if cr not in ref: continue\n",
    "        peak = (start + end) / 2\n",
    "        genes, genebody, genes0, genebody0 = [], [], [], []\n",
    "        for name, symbol, strand, TSS, TES in ref[cr]:\n",
    "            if strand == '+':\n",
    "                dist = end - TSS\n",
    "                if abs(start-TSS)<abs(dist):dist=start-TSS\n",
    "\n",
    "            elif strand == '-':\n",
    "                dist = TSS - end\n",
    "                if abs(TSS - start)<abs(dist):dist=TSS - start\n",
    "\n",
    "            elif strand != 'none': raise ValueError\n",
    "            if abs(dist) <= cutoff: genes.append((abs(dist),symbol,dist)) \n",
    "            if (start - TSS) * (start - TES) <= 0: genebody.append((abs(dist),symbol,dist))\n",
    "            elif (end - TSS) * (end - TES) <= 0: genebody.append((abs(dist),symbol,dist))\n",
    "            elif (start - TSS) * (end - TES) <=0: genebody.append((abs(dist),symbol,dist))\n",
    "        genes, genebody = sorted(genes), sorted(genebody)\n",
    "        for g in sorted(genes):\n",
    "            if g[1] not in [x[1] for x in genes0]: genes0.append(g)\n",
    "        for g in sorted(genebody):\n",
    "            if g[1] not in [x[1] for x in genebody0]: genebody0.append(g)\n",
    "        if any(genes0):\n",
    "            symbols = ','.join([x[1] for x in genes0])\n",
    "            dists = ','.join(['%d' % x[2] for x in sorted(genes0)])\n",
    "        else: symbols, dists = 'none', 'none'\n",
    "        if any(genebody): body = ','.join([x[1] for x in genebody0])\n",
    "        else: body = 'none'\n",
    "        fout.write(line[:-1] + '\\t%s\\t%s\\t%s\\n' % (body,symbols,dists))\n",
    "    fout.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_ENL.nsp_peaks.anno.1k.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_ENL.nsp_peaks.anno.3k.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_ENL.nsp_peaks.anno.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_T1.nsp_peaks.anno.1k.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_T1.nsp_peaks.anno.3k.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_T1.nsp_peaks.anno.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_T2.nsp_peaks.anno.1k.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_T2.nsp_peaks.anno.3k.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_T2.nsp_peaks.anno.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_T3.nsp_peaks.anno.1k.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_T3.nsp_peaks.anno.3k.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_T3.nsp_peaks.anno.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_Y78A.nsp_peaks.anno.1k.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_Y78A.nsp_peaks.anno.3k.txt\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda//293_F_Y78A.nsp_peaks.anno.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls $outpath/*.anno*.txt\n",
    "# !tail /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_ENL.nsp_peaks.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(f'/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2*nsp_peaks.bed')\n",
    "outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing on /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_T2.nsp_peaks.bed\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_F_T2.nsp_peaks.anno.txt\n",
      "\n",
      "processing on /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_Y78A.nsp_peaks.bed\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_F_Y78A.nsp_peaks.anno.txt\n",
      "\n",
      "processing on /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_T1.nsp_peaks.bed\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_F_T1.nsp_peaks.anno.txt\n",
      "\n",
      "processing on /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_T3.nsp_peaks.bed\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_F_T3.nsp_peaks.anno.txt\n",
      "\n",
      "processing on /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_ENL.nsp_peaks.bed\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_F_ENL.nsp_peaks.anno.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files_to_annoate = glob.glob(f'/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2*nsp_peaks.bed')\n",
    "files_to_annoate\n",
    "infile = files_to_annoate[0]\n",
    "for infile in files_to_annoate:\n",
    "    outfile = outpath + infile.split('/')[-1].split('.bed')[0] + '.anno.txt'\n",
    "    annoate_bed_to_gene(infile, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the signal for the peaks\n",
    "2018-7-5\n",
    "bed files locates: /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/\n",
    "bw files locates: /home/xc3/output/ENL2/ChIP/bw_nsp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. merge the bedfiles together for Treat and wildtype (ENL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = ['293', 'HK2']\n",
    "bwpath = '/mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/'\n",
    "exp = exps[1]\n",
    "treats = ['T1', 'T2', 'T3', 'Y78A']\n",
    "wt = 'ENL'\n",
    "inpath = '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/'\n",
    "outpath = './2.Re_do_with_lambda_macs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge a list of the bed files\n",
    "# depeand on the bedtools\n",
    "def merge_beds(bed_files, out_name, header=True):\n",
    "    \"\"\"\n",
    "    bed_files: a list of bed files to be merged\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for bed_file in bed_files:\n",
    "        with open(bed_file, 'r') as f:\n",
    "            lines.extend(f.readlines()[1:] if header else f.readlines())\n",
    "    # merge the two beds together        \n",
    "    with open('tmp.bed','w') as f:\n",
    "        f.writelines(lines)\n",
    "    \n",
    "    # sort the files\n",
    "    command = \"sort -k1,1 -k2,2n tmp.bed > tmp.sorted.bed\"\n",
    "    print(f'execute {command}')\n",
    "    os.system(command)\n",
    "    \n",
    "    # merge the files\n",
    "    command = f\"bedtools merge -i tmp.sorted.bed -d 1 > {out_name}\"\n",
    "    print(f'execute {command}')\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_T1.nsp_peaks.bed', '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_ENL.nsp_peaks.bed']\n",
      "execute sort -k1,1 -k2,2n tmp.bed > tmp.sorted.bed\n",
      "execute bedtools merge -i tmp.sorted.bed -d 1 > ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.bed\n",
      "['/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_T2.nsp_peaks.bed', '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_ENL.nsp_peaks.bed']\n",
      "execute sort -k1,1 -k2,2n tmp.bed > tmp.sorted.bed\n",
      "execute bedtools merge -i tmp.sorted.bed -d 1 > ./2.Re_do_with_lambda_macs/HK2_T2_WT.merge.bed\n",
      "['/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_T3.nsp_peaks.bed', '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_ENL.nsp_peaks.bed']\n",
      "execute sort -k1,1 -k2,2n tmp.bed > tmp.sorted.bed\n",
      "execute bedtools merge -i tmp.sorted.bed -d 1 > ./2.Re_do_with_lambda_macs/HK2_T3_WT.merge.bed\n",
      "['/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_Y78A.nsp_peaks.bed', '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_ENL.nsp_peaks.bed']\n",
      "execute sort -k1,1 -k2,2n tmp.bed > tmp.sorted.bed\n",
      "execute bedtools merge -i tmp.sorted.bed -d 1 > ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.bed\n"
     ]
    }
   ],
   "source": [
    "#293_F_T2.nsp_peaks.bed\n",
    "# for exp in exps:\n",
    "for treat in treats:\n",
    "    treat_bed = inpath + f'{exp}_F_{treat}.nsp_peaks.bed'\n",
    "    wt_bed = inpath + f'{exp}_F_ENL.nsp_peaks.bed'\n",
    "    bed_files = [treat_bed, wt_bed]\n",
    "    print(bed_files)\n",
    "    out_name = outpath + f'{exp}_{treat}_WT.merge.bed'\n",
    "    merge_beds(bed_files, out_name=out_name, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for formating the chr start end into bed format\n",
    "# if the last column has sign ('+' or '-'), then prepare to swap tss and tes for '-'\n",
    "def convert_region_to_bed(bedfile, outname, header=None):\n",
    "    df = pd.read_csv(bedfile,sep='\\t',header=None)\n",
    "    df.columns = np.arange(0,df.shape[1])\n",
    "    # detect the strand and swap the start end when negative strand\n",
    "    # the last column should be strand\n",
    "    if df.iloc[0,-1] == '-' or df.iloc[0,-1] == '+':\n",
    "        idx = df.iloc[:,-1] == '-'\n",
    "        # swap the tss tes according to the strand\n",
    "        df.loc[idx,[1,2]] = df.loc[idx,[2,1]].values\n",
    "        # modify the strand\n",
    "        df.iloc[:,-1] = '+'\n",
    "        \n",
    "    df.loc[:,3] = np.arange(df.shape[0])# for gene name column\n",
    "    df.loc[:,4] = 0 # for bed format (value)\n",
    "    df.loc[:,5] = '+'\n",
    "    df = df.reindex(range(6), axis='columns')\n",
    "    print('writing the output --> {}'.format(outname))\n",
    "    print(f'{outname} --->: reformated bed_file as the input of the bigWigAverageoverBed')\n",
    "    df.to_csv(outname,sep='\\t',index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_avg_sig_from_bw_by_bed(bedfile, bw_files, which_column=2):\n",
    "    \"\"\"\n",
    "    bedfile: the input merged bedfile which needs to extract the signal from\n",
    "    bw_files: list of bw files whose signal is extracted from\n",
    "    which_column: 3 is the acutal counts column, 4 is the acutal avg signal column;\n",
    "    but, we need to use the first column as index, then 2 is the counts column;\n",
    "    outname: reformat the bedfile to the normal 6 columns bed file\n",
    "    will save the output file in the same folder as the bedfile\n",
    "    \"\"\"\n",
    "    outname = bedfile.split('.bed')[0] + '.str.bed'\n",
    "    # convert the 3 columns bed to the normal bed file\n",
    "    convert_region_to_bed(bedfile, outname)\n",
    "    for i,bw_file in enumerate(bw_files):\n",
    "        command = line = f'~/xc3/software/bigWigAverageOverBed {bw_file} {outname} avg_sig_{i}.txt'\n",
    "        print('execute --> {}'.format(command))\n",
    "        os.system(command)\n",
    "    dfs = []\n",
    "    for i in range(len(bw_files)):\n",
    "        dfs.append(pd.read_csv(f'avg_sig_{i}.txt',sep='\\t',header=None, index_col=0))\n",
    "    df = pd.DataFrame([dfs[0].iloc[:,0]] + # iloc[:,0]: the width\n",
    "                      [dfi.iloc[:,which_column] for dfi in dfs]).T # conbine the columns together\n",
    "    df.columns = ['width'] + [bw_file.split('/')[-1].split('.')[0] for bw_file in bw_files]\n",
    "    df_bed = pd.read_csv(bedfile, sep='\\t', header=None, names = ['chr', 'start', 'end'])\n",
    "    df_out = df_bed.join(df)\n",
    "    print(f\"output file -->: {bedfile.split('.bed')[0] + '.counts.txt'}\")\n",
    "    df_out.to_csv(bedfile.split('.bed')[0] + '.counts.txt', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "treats = ['T1', 'T2', 'T3', 'Y78A']\n",
    "treats = ['T1', 'Y78A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. extract the signal\n",
    "merged bed files : out_name = outpath + f'{exp}_{treat}_WT.merge.bed'\n",
    "'./res_avg_signal/293_T1_WT.merge.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bw_files--->:\n",
      "/mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_T1_treat.nsp.bw|/mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_ENL_treat.nsp.bw|/mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_ctrl.nsp.bw\n",
      "outname--->: ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.test.bed\n",
      "writing the output --> ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.test.bed\n",
      "./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.test.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "writing the output --> ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.str.bed\n",
      "./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.str.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "execute --> ~/xc3/software/bigWigAverageOverBed /mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_T1_treat.nsp.bw ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.str.bed avg_sig_0.txt\n",
      "execute --> ~/xc3/software/bigWigAverageOverBed /mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_ENL_treat.nsp.bw ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.str.bed avg_sig_1.txt\n",
      "execute --> ~/xc3/software/bigWigAverageOverBed /mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_ctrl.nsp.bw ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.str.bed avg_sig_2.txt\n",
      "output file -->: ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.counts.txt\n",
      "bw_files--->:\n",
      "/mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_Y78A_treat.nsp.bw|/mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_ENL_treat.nsp.bw|/mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_ctrl.nsp.bw\n",
      "outname--->: ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.test.bed\n",
      "writing the output --> ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.test.bed\n",
      "./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.test.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "writing the output --> ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.str.bed\n",
      "./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.str.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "execute --> ~/xc3/software/bigWigAverageOverBed /mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_Y78A_treat.nsp.bw ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.str.bed avg_sig_0.txt\n",
      "execute --> ~/xc3/software/bigWigAverageOverBed /mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_ENL_treat.nsp.bw ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.str.bed avg_sig_1.txt\n",
      "execute --> ~/xc3/software/bigWigAverageOverBed /mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_ctrl.nsp.bw ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.str.bed avg_sig_2.txt\n",
      "output file -->: ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.counts.txt\n"
     ]
    }
   ],
   "source": [
    "for treat in treats:\n",
    "    bedfile = outpath + f'{exp}_{treat}_WT.merge.bed'\n",
    "    df_bed = pd.read_csv(bedfile, sep='\\t', header=None, names = ['chr', 'start', 'end'])\n",
    "    bw_files = [glob.glob(bwpath+exp+'*'+tmp+'*.bw')[0] for tmp in [treat,'ENL', 'ctrl']]\n",
    "    print(f'bw_files--->:\\n{\"|\".join(bw_files)}')\n",
    "    outname = bedfile.split('.bed')[0] + '.test.bed'\n",
    "    print(f'outname--->: {outname}')\n",
    "    convert_region_to_bed(bedfile, outname) #format the bedfile as outname, especially for the bigWigAve\n",
    "    comp_avg_sig_from_bw_by_bed(bedfile, bw_files, which_column=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bw_files--->:\n",
      "/mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_Y78A_treat.nsp.bw|/mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_ENL_treat.nsp.bw|/mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_ctrl.nsp.bw\n",
      "outname--->: ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.test.bed\n",
      "writing the output --> ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.test.bed\n",
      "./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.test.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "writing the output --> ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.str.bed\n",
      "./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.str.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "execute --> ~/xc3/software/bigWigAverageOverBed /mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_Y78A_treat.nsp.bw ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.str.bed avg_sig_0.txt\n",
      "execute --> ~/xc3/software/bigWigAverageOverBed /mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_F_ENL_treat.nsp.bw ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.str.bed avg_sig_1.txt\n",
      "execute --> ~/xc3/software/bigWigAverageOverBed /mount/weili2/lilab/xc3/ENL2/ChIP/bw_nsp/HK2_ctrl.nsp.bw ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.str.bed avg_sig_2.txt\n",
      "output file -->: ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.counts.txt\n",
      "results --> \n",
      ":     chr   start     end   width  HK2_F_Y78A_treat  HK2_F_ENL_treat  HK2_ctrl  \\\n",
      "0  chr1  804921  805827   906.0            8133.0          10400.0    2578.0   \n",
      "1  chr1  845409  847293  1884.0           15365.0          24953.0    7927.0   \n",
      "2  chr1  858673  861453  2780.0           34485.0          53235.0   11258.0   \n",
      "3  chr1  873327  874921  1594.0           14599.0          21452.0    7123.0   \n",
      "4  chr1  875255  877832  2577.0           28967.0          46983.0    9254.0   \n",
      "\n",
      "   width_val  \n",
      "0        906  \n",
      "1       1884  \n",
      "2       2780  \n",
      "3       1594  \n",
      "4       2577  \n",
      "10600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>width</th>\n",
       "      <th>HK2_F_Y78A_treat</th>\n",
       "      <th>HK2_F_ENL_treat</th>\n",
       "      <th>HK2_ctrl</th>\n",
       "      <th>width_val</th>\n",
       "      <th>validataion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [chr, start, end, width, HK2_F_Y78A_treat, HK2_F_ENL_treat, HK2_ctrl, width_val, validataion]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### tester code\n",
    "exp = 'HK2'\n",
    "bedfile = './2.Re_do_with_lambda_macs/HK2_T1_WT.merge.bed'\n",
    "df_bed = pd.read_csv(bedfile, sep='\\t', header=None, names = ['chr', 'start', 'end'])\n",
    "bw_files = [glob.glob(bwpath+exp+'*'+tmp+'*.bw')[0] for tmp in [treat,'ENL', 'ctrl']]\n",
    "print(f'bw_files--->:\\n{\"|\".join(bw_files)}')\n",
    "outname = bedfile.split('.bed')[0] + '.test.bed'\n",
    "print(f'outname--->: {outname}')\n",
    "convert_region_to_bed(bedfile, outname) #format the bedfile as outname, especially for the bigWigAve\n",
    "comp_avg_sig_from_bw_by_bed(bedfile, bw_files)\n",
    "df = pd.read_csv(bedfile.split('.bed')[0] + '.counts.txt', sep='\\t')\n",
    "df['width_val'] = df['end'] - df['start']\n",
    "print(f'results --> \\n: {df.head()}')\n",
    "df['validataion'] = df['width_val'] == df['width'].values.astype(int)\n",
    "print(sum(df['validataion']))\n",
    "df[df['validataion']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Annotate the signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing on ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.counts.txt\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.counts.anno.txt\n",
      "\n",
      "processing on ./2.Re_do_with_lambda_macs/HK2_T2_WT.merge.counts.txt\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_T2_WT.merge.counts.anno.txt\n",
      "\n",
      "processing on ./2.Re_do_with_lambda_macs/HK2_T3_WT.merge.counts.txt\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_T3_WT.merge.counts.anno.txt\n",
      "\n",
      "processing on ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.counts.txt\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.counts.anno.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.txt')\n",
    "files_to_annoate\n",
    "infile = files_to_annoate[0]\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.txt')[0] + '.anno.txt'\n",
    "    annoate_bed_to_gene(infile, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate the signal for each peak\n",
    "convert the count files to sig files and store them seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr\tstart\tend\twidth\tHK2_F_Y78A_treat\tHK2_F_ENL_treat\tHK2_ctrl\twithin_genebody\tnearest_TSS\tdistance\r\n",
      "chr1\t804921\t805749\t828\t7271\t9846\t2373\tFAM41C\tnone\tnone\r\n"
     ]
    }
   ],
   "source": [
    "!head ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.counts.anno.txt -n2\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.txt')\n",
    "files_to_annoate\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.counts.txt')[0] + '.sig.txt'\n",
    "    df = pd.read_csv(infile, header=0, sep='\\t')\n",
    "    df.iloc[:,-3:] = df.iloc[:,-3:].div(df.iloc[:,3],axis=0)\n",
    "    df.to_csv(outfile, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing on ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.sig.txt\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.sig.anno.txt\n",
      "\n",
      "processing on ./2.Re_do_with_lambda_macs/HK2_T2_WT.merge.sig.txt\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_T2_WT.merge.sig.anno.txt\n",
      "\n",
      "processing on ./2.Re_do_with_lambda_macs/HK2_T3_WT.merge.sig.txt\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_T3_WT.merge.sig.anno.txt\n",
      "\n",
      "processing on ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.sig.txt\n",
      " will output ./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.sig.anno.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## annotate the signal file with the gene annotation\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.sig.txt')\n",
    "files_to_annoate\n",
    "infile = files_to_annoate[0]\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.txt')[0] + '.anno.txt'\n",
    "    annoate_bed_to_gene(infile, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate the counts for edgeR\n",
    "write the count for each paired treat versus ctrl and do the differential analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. generate the files with counts\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.txt')\n",
    "files_to_annoate\n",
    "for infile in files_to_annoate:\n",
    "    outfile_treat = infile.split('.counts.txt')[0] + '.t.txt'\n",
    "    outfile_ctrl = infile.split('.counts.txt')[0] + '.c.txt'\n",
    "    df = pd.read_csv(infile, header=0, sep='\\t')\n",
    "    df.iloc[:,4].to_csv(outfile_treat, sep='\\t',index=True,header=False)\n",
    "    df.iloc[:,5].to_csv(outfile_ctrl, sep='\\t',index=True,header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. write the code for differential analysis\n",
    "# edgeR template should run in the expression folder\n",
    "# run the sh\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.txt')\n",
    "# print(files_to_annoate)\n",
    "with open('de_pval_counts.sh', 'w') as f:\n",
    "    for infile in files_to_annoate:\n",
    "#         print(infile)\n",
    "        outfile_treat = infile.split('/')[-1].split('.counts.txt')[0] + '.t.txt'\n",
    "        outfile_ctrl = infile.split('/')[-1].split('.counts.txt')[0] + '.c.txt'\n",
    "        outfile = infile.split('/')[-1].split('.counts.txt')[0] + '.pval.txt'\n",
    "        line = f'python /home/jlyu/xc3/software/edgeR.py -s /home/jlyu/xc3/software/edgeR.template -q 1 -t both {outfile_treat}:1:2 {outfile_ctrl}:1:2 {outfile}'\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.counts.anno.pval.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_T2_WT.merge.counts.anno.pval.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_T3_WT.merge.counts.anno.pval.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.counts.anno.pval.txt\n"
     ]
    }
   ],
   "source": [
    "# 3. combine the counts and pval together with counts files\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.anno.txt')\n",
    "files_to_annoate\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.txt')[0] + '.pval.txt'\n",
    "    pval_file = infile.split('.counts')[0] + '.pval.txt'\n",
    "    df_pval = pd.read_csv(pval_file, sep='\\t', header=0, index_col='id')\n",
    "    # according to the index to sort the df_pval\n",
    "    df_pval.sort_index(axis=0, inplace=True)\n",
    "    df = pd.read_csv(infile, sep='\\t', header=0)\n",
    "    df_comb = pd.concat([df, df_pval], axis=1)\n",
    "    df_comb.iloc[:, :-3].to_csv(outfile, sep='\\t', index=False)\n",
    "    print(outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.sig.anno.pval.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_T2_WT.merge.sig.anno.pval.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_T3_WT.merge.sig.anno.pval.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.sig.anno.pval.txt\n"
     ]
    }
   ],
   "source": [
    "# 3. combine the sig and pval together with counts files\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.sig.anno.txt')\n",
    "files_to_annoate\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.txt')[0] + '.pval.txt'\n",
    "    pval_file = infile.split('.sig')[0] + '.pval.txt'\n",
    "    df_pval = pd.read_csv(pval_file, sep='\\t', header=0, index_col='id')\n",
    "    # according to the index to sort the df_pval\n",
    "    df_pval.sort_index(axis=0, inplace=True)\n",
    "    df = pd.read_csv(infile, sep='\\t', header=0)\n",
    "    df_comb = pd.concat([df, df_pval], axis=1)\n",
    "    df_comb.iloc[:, :-3].to_csv(outfile, sep='\\t', index=False)\n",
    "    print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.counts.anno.genes1_2.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_T1_WT.merge.counts.anno.genes1_5.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_T2_WT.merge.counts.anno.genes1_2.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_T2_WT.merge.counts.anno.genes1_5.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_T3_WT.merge.counts.anno.genes1_2.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_T3_WT.merge.counts.anno.genes1_5.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.counts.anno.genes1_2.txt\n",
      "./2.Re_do_with_lambda_macs/HK2_Y78A_WT.merge.counts.anno.genes1_5.txt\n"
     ]
    }
   ],
   "source": [
    "# 4. extracted the gene lists with fold change for 1.2 and 1.5\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.anno.txt')\n",
    "files_to_annoate\n",
    "for infile in files_to_annoate:\n",
    "    outfile1 = infile.split('.txt')[0] + '.genes1_2.txt'\n",
    "    outfile2 = infile.split('.txt')[0] + '.genes1_5.txt'\n",
    "    df = pd.read_csv(infile, sep='\\t', header=0)\n",
    "    df['fc'] = df.iloc[:,4].div(df.iloc[:,5])\n",
    "    # combine the two dataframes into the list\n",
    "    dfs_list = [df[df['fc'] >= 1.2], df[df['fc'] >= 1.5]]\n",
    "    # combine the outfiles names into the list \n",
    "    outfiles = [outfile1, outfile2]\n",
    "    # extract the genes\n",
    "    for tmp, outfile in zip(dfs_list, outfiles):\n",
    "        body_tss = []\n",
    "        body = []\n",
    "        tss = []\n",
    "        for cols in tmp.iloc[:,7]:\n",
    "            eles = cols.split(',')\n",
    "            for ele in eles:\n",
    "                if ele and ele != 'none':\n",
    "                    body.append(ele)\n",
    "\n",
    "        for cols in tmp.iloc[:,8]:\n",
    "            eles = cols.split(',')\n",
    "            for ele in eles:\n",
    "                if ele and ele != 'none':\n",
    "                    tss.append(ele)\n",
    "\n",
    "        body.extend(tss)\n",
    "        body = list(set(body))\n",
    "        with open(outfile, 'w') as f:\n",
    "            f.writelines('\\n'.join(body))\n",
    "            \n",
    "        print(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_annoate = glob.glob(f'{outpath}*.counts.anno.txt')\n",
    "files_to_annoate\n",
    "infile = files_to_annoate[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9607, 11)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(infile, sep='\\t', header=0)\n",
    "df['fc'] = df.iloc[:,4].div(df.iloc[:,5])\n",
    "dfs_list = [df[df['fc'] >= 1.2], df[df['fc'] >= 1.5]]\n",
    "dfs_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
