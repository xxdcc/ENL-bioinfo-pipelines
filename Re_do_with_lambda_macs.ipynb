{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the pbs parameters for macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the parameters\n",
    "nodes_ppn = [1, 1]\n",
    "memory = 16\n",
    "wait_time = [20, 00]\n",
    "quene = \"medium\"\n",
    "inpath = '/mount/weili3/xc3/ENL2_ChIP/data/'\n",
    "exps = ['293*', 'HK2*']\n",
    "ctrl = 'input*'\n",
    "treat = 'F*'\n",
    "outpath = '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/'\n",
    "exp = exps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the batches of bash jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_ENL.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_ENL.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T1.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T1.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T2.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T2.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T3.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_T3.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_Y78A.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/293_F_Y78A.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_ENL.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_ENL.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T1.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T1.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T2.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T2.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T3.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_T3.nsp_treat_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_Y78A.nsp_control_afterfiting_all.wig.gz\r\n",
      "/mount/weili3/xc3/ENL2_ChIP/res_nsp/HK2_F_Y78A.nsp_treat_afterfiting_all.wig.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls /mount/weili3/xc3/ENL2_ChIP/res_nsp/*.wig.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mount/weili3/xc3/ENL2_ChIP/data/293_F_T1/293_F_T1.bowtie.bed.nsp.shuf', '/mount/weili3/xc3/ENL2_ChIP/data/293_F_T2/293_F_T2.bowtie.bed.nsp.shuf', '/mount/weili3/xc3/ENL2_ChIP/data/293_F_T3/293_F_T3.bowtie.bed.nsp.shuf', '/mount/weili3/xc3/ENL2_ChIP/data/293_F_Y78A/293_F_Y78A.bowtie.bed.nsp.shuf', '/mount/weili3/xc3/ENL2_ChIP/data/293_F_ENL/293_F_ENL.bowtie.bed.nsp.shuf']\n",
      "/mount/weili3/xc3/ENL2_ChIP/data/293_input_S32/293_input_S32.bowtie.bed.nsp.shuf\n"
     ]
    }
   ],
   "source": [
    "treat_files = glob.glob(inpath+exp+treat+'/*.bowtie.bed.nsp.shuf')\n",
    "ctrl_file = glob.glob(inpath+exp+ctrl+'/*.bowtie.bed.nsp.shuf')[0]\n",
    "print(treat_files)\n",
    "print(ctrl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the initialization header for pbs and read the contents\n",
    "with open('/home/xc3/experiment/initial.pbs','r') as f:\n",
    "\tpbs_header = f.readlines()\n",
    "\n",
    "for exp in exps:\n",
    "\t\n",
    "\ttreat_files = glob.glob(inpath+exp+treat+'/*.bowtie.bed.nsp.shuf')\n",
    "\tctrl_file = glob.glob(inpath+exp+ctrl+'/*.bowtie.bed.nsp.shuf')[0]\n",
    "\tnames = [ifile.split('/')[-2] for ifile in treat_files]\n",
    "\n",
    "\tfor i, treat_file in enumerate(treat_files):\n",
    "\t\texperiment_name = \"macs_{}.ato\".format(names[i])\n",
    "\t\twith open(experiment_name,'w') as f:\n",
    "\n",
    "\t\t\tpbs_initial = pbs_header[:]\n",
    "\t\t\t# configuration for the experiments\n",
    "\t\t\t# pbs_initial[1]::job name\n",
    "\t\t\tpbs_initial[1] = pbs_initial[1].format(str(names[i]))\n",
    "\t\t\t# pbs_initial[2]::nodes and ppn\n",
    "\t\t\tpbs_initial[2] = pbs_initial[2].format(*nodes_ppn)\n",
    "\t\t\t# pbs_initial[4]::memeroy\n",
    "\t\t\tpbs_initial[4] = pbs_initial[4].format(memory)\n",
    "\t\t\t# pbs_initial[5]::waiting time\n",
    "\t\t\tpbs_initial[5] = pbs_initial[5].format(*wait_time)\n",
    "\t\t\t# pbs_initial[11]::err output\n",
    "\t\t\tpbs_initial[11] = pbs_initial[11].format(names[i])\n",
    "\t\t\t# pbs_initial[12]::log\n",
    "\t\t\tpbs_initial[12] = pbs_initial[12].format(names[i])\n",
    "\t\t\t# pbs_initial[14]::quene\n",
    "\t\t\tpbs_initial[14] = pbs_initial[14].format(quene)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t# write all the configurations into the pbs file\n",
    "\t\t\tfor line in pbs_initial:\n",
    "\t\t\t\tf.write(line)\n",
    "\n",
    "\t\t\t# write the shuf code\n",
    "\t\t\tline =\"macs14 -t {} -c {} -n {}.nsp --nomodel -g hs --wig -S -p 1e-8\".format(treat_files[i], ctrl_file, outpath + names[i])\n",
    "\t\t\t\n",
    "\t\t\tf.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotated the above peaks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annoate_bed_to_gene(infile, outfile, cutoff=5000):\n",
    "    ref, tss_set = {}, set()\n",
    "    reffile = '/mount/weili3/xc3/genomes/hg19.refGene.txt'\n",
    "    for line in open(reffile):\n",
    "        col = line.split('\\t')\n",
    "        name, cr, strand, TSS, TES, symbol = col[1], col[2], col[3], int(col[4]), int(col[5]), col[12]\n",
    "        if strand == '-': TSS, TES = TES, TSS\n",
    "        if cr not in ref: ref[cr] = []\n",
    "        if (cr,TSS,strand) not in tss_set:\n",
    "            ref[cr].append((name,symbol,strand,TSS,TES))\n",
    "            tss_set.add((cr,TSS,strand))\n",
    "    for cr in ref: ref[cr].append(('none','none','none',0,0))\n",
    "    \n",
    "    # annotate the file\n",
    "    text = open(infile).readlines()\n",
    "    fout = open(outfile, 'w')\n",
    "    print('processing on {}\\n will output {}\\n'.format(infile,outfile))\n",
    "    for line in text:\n",
    "        col = line.split('\\t')\n",
    "        try: cr, start, end = col[0], int(col[1]), int(col[2])\n",
    "        except: \n",
    "            fout.write(line[:-1]+'\\twithin_genebody\\tnearest_TSS\\tdistance\\n')\n",
    "            continue\n",
    "        if cr not in ref: continue\n",
    "        peak = (start + end) / 2\n",
    "        genes, genebody, genes0, genebody0 = [], [], [], []\n",
    "        for name, symbol, strand, TSS, TES in ref[cr]:\n",
    "            if strand == '+':\n",
    "                dist = end - TSS\n",
    "                if abs(start-TSS)<abs(dist):dist=start-TSS\n",
    "\n",
    "            elif strand == '-':\n",
    "                dist = TSS - end\n",
    "                if abs(TSS - start)<abs(dist):dist=TSS - start\n",
    "\n",
    "            elif strand != 'none': raise ValueError\n",
    "            if abs(dist) <= cutoff: genes.append((abs(dist),symbol,dist)) \n",
    "            if (start - TSS) * (start - TES) <= 0: genebody.append((abs(dist),symbol,dist))\n",
    "            elif (end - TSS) * (end - TES) <= 0: genebody.append((abs(dist),symbol,dist))\n",
    "            elif (start - TSS) * (end - TES) <=0: genebody.append((abs(dist),symbol,dist))\n",
    "        genes, genebody = sorted(genes), sorted(genebody)\n",
    "        for g in sorted(genes):\n",
    "            if g[1] not in [x[1] for x in genes0]: genes0.append(g)\n",
    "        for g in sorted(genebody):\n",
    "            if g[1] not in [x[1] for x in genebody0]: genebody0.append(g)\n",
    "        if any(genes0):\n",
    "            symbols = ','.join([x[1] for x in genes0])\n",
    "            dists = ','.join(['%d' % x[2] for x in sorted(genes0)])\n",
    "        else: symbols, dists = 'none', 'none'\n",
    "        if any(genebody): body = ','.join([x[1] for x in genebody0])\n",
    "        else: body = 'none'\n",
    "        fout.write(line[:-1] + '\\t%s\\t%s\\t%s\\n' % (body,symbols,dists))\n",
    "    fout.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293_T1_WT.merge.bed  293_T3_WT.merge.bed\r\n",
      "293_T2_WT.merge.bed  293_Y78A_WT.merge.bed\r\n"
     ]
    }
   ],
   "source": [
    "!ls $outpath\n",
    "# !tail /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_ENL.nsp_peaks.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_annoate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing on /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_Y78A.nsp_peaks.bed\n",
      " will output /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_Y78A.nsp_peaks.anno.txt\n",
      "\n",
      "processing on /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_T2.nsp_peaks.bed\n",
      " will output /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_T2.nsp_peaks.anno.txt\n",
      "\n",
      "processing on /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_ENL.nsp_peaks.bed\n",
      " will output /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_ENL.nsp_peaks.anno.txt\n",
      "\n",
      "processing on /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_T1.nsp_peaks.bed\n",
      " will output /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_T1.nsp_peaks.anno.txt\n",
      "\n",
      "processing on /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_T3.nsp_peaks.bed\n",
      " will output /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_T3.nsp_peaks.anno.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files_to_annoate = glob.glob(f'{outpath}*nsp_peaks.bed')\n",
    "files_to_annoate\n",
    "infile = files_to_annoate[0]\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.bed')[0] + '.anno.txt'\n",
    "    annoate_bed_to_gene(infile, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the signal for the peaks\n",
    "2018-7-5\n",
    "bed files locates: /mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/\n",
    "bw files locates: /home/xc3/output/ENL2/ChIP/bw_nsp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. merge the bedfiles together for Treat and wildtype (ENL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = ['293', 'HK2']\n",
    "bwpath = '/home/xc3/output/ENL2/ChIP/bw_nsp/'\n",
    "exp = exps[0]\n",
    "treats = ['T1', 'T2', 'T3', 'Y78A']\n",
    "wt = 'ENL'\n",
    "inpath = '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/'\n",
    "outpath = './res_avg_signal/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge a list of the bed files\n",
    "# depeand on the bedtools\n",
    "def merge_beds(bed_files, out_name, header=True):\n",
    "    \"\"\"\n",
    "    bed_files: a list of bed files to be merged\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for bed_file in bed_files:\n",
    "        with open(bed_file, 'r') as f:\n",
    "            lines.extend(f.readlines()[1:] if header else f.readlines())\n",
    "    # merge the two beds together        \n",
    "    with open('tmp.bed','w') as f:\n",
    "        f.writelines(lines)\n",
    "    \n",
    "    # sort the files\n",
    "    command = \"sort -k1,1 -k2,2n tmp.bed > tmp.sorted.bed\"\n",
    "    print(f'execute {command}')\n",
    "    os.system(command)\n",
    "    \n",
    "    # merge the files\n",
    "    command = f\"bedtools merge -i tmp.sorted.bed -d 1 > {out_name}\"\n",
    "    print(f'execute {command}')\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_T1.nsp_peaks.bed', '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_ENL.nsp_peaks.bed']\n",
      "execute sort -k1,1 -k2,2n tmp.bed > tmp.sorted.bed\n",
      "execute bedtools merge -i tmp.sorted.bed -d 1 > ./res_avg_signal/293_T1_WT.merge.bed\n",
      "['/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_T2.nsp_peaks.bed', '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_ENL.nsp_peaks.bed']\n",
      "execute sort -k1,1 -k2,2n tmp.bed > tmp.sorted.bed\n",
      "execute bedtools merge -i tmp.sorted.bed -d 1 > ./res_avg_signal/293_T2_WT.merge.bed\n",
      "['/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_T3.nsp_peaks.bed', '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_ENL.nsp_peaks.bed']\n",
      "execute sort -k1,1 -k2,2n tmp.bed > tmp.sorted.bed\n",
      "execute bedtools merge -i tmp.sorted.bed -d 1 > ./res_avg_signal/293_T3_WT.merge.bed\n",
      "['/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_Y78A.nsp_peaks.bed', '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/293_F_ENL.nsp_peaks.bed']\n",
      "execute sort -k1,1 -k2,2n tmp.bed > tmp.sorted.bed\n",
      "execute bedtools merge -i tmp.sorted.bed -d 1 > ./res_avg_signal/293_Y78A_WT.merge.bed\n",
      "['/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_T1.nsp_peaks.bed', '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_ENL.nsp_peaks.bed']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_T1.nsp_peaks.bed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-1404bce2a194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbed_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'{exp}_{treat}_WT.merge.bed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmerge_beds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbed_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-c4354c6be207>\u001b[0m in \u001b[0;36mmerge_beds\u001b[0;34m(bed_files, out_name, header)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbed_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbed_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbed_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# merge the two beds together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mount/weili3/xc3/ENL2_ChIP/res_nsp_lambda/HK2_F_T1.nsp_peaks.bed'"
     ]
    }
   ],
   "source": [
    "#293_F_T2.nsp_peaks.bed\n",
    "# for exp in exps:\n",
    "    for treat in treats:\n",
    "        treat_bed = inpath + f'{exp}_F_{treat}.nsp_peaks.bed'\n",
    "        wt_bed = inpath + f'{exp}_F_ENL.nsp_peaks.bed'\n",
    "        bed_files = [treat_bed, wt_bed]\n",
    "        print(bed_files)\n",
    "        out_name = outpath + f'{exp}_{treat}_WT.merge.bed'\n",
    "        merge_beds(bed_files, out_name=out_name, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for formating the chr start end into bed format\n",
    "# if the last column has sign ('+' or '-'), then prepare to swap tss and tes for '-'\n",
    "def convert_region_to_bed(bedfile, outname, header=None):\n",
    "    df = pd.read_csv(bedfile,sep='\\t',header=None)\n",
    "    df.columns = np.arange(0,df.shape[1])\n",
    "    # detect the strand and swap the start end when negative strand\n",
    "    # the last column should be strand\n",
    "    if df.iloc[0,-1] == '-' or df.iloc[0,-1] == '+':\n",
    "        idx = df.iloc[:,-1] == '-'\n",
    "        # swap the tss tes according to the strand\n",
    "        df.loc[idx,[1,2]] = df.loc[idx,[2,1]].values\n",
    "        # modify the strand\n",
    "        df.iloc[:,-1] = '+'\n",
    "        \n",
    "    df.loc[:,3] = np.arange(df.shape[0])# for gene name column\n",
    "    df.loc[:,4] = 0 # for bed format (value)\n",
    "    df.loc[:,5] = '+'\n",
    "    df = df.reindex(range(6), axis='columns')\n",
    "    print('writing the output --> {}'.format(outname))\n",
    "    print(f'{outname} --->: reformated bed_file as the input of the bigWigAverageoverBed')\n",
    "    df.to_csv(outname,sep='\\t',index=False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_avg_sig_from_bw_by_bed(bedfile, bw_files, which_column=2):\n",
    "    \"\"\"\n",
    "    bedfile: the input merged bedfile which needs to extract the signal from\n",
    "    bw_files: list of bw files whose signal is extracted from\n",
    "    which_column: 3 is the acutal counts column, 4 is the acutal avg signal column;\n",
    "    but, we need to use the first column as index, then 2 is the counts column;\n",
    "    outname: reformat the bedfile to the normal 6 columns bed file\n",
    "    will save the output file in the same folder as the bedfile\n",
    "    \"\"\"\n",
    "    outname = bedfile.split('.bed')[0] + '.str.bed'\n",
    "    # convert the 3 columns bed to the normal bed file\n",
    "    convert_region_to_bed(bedfile, outname)\n",
    "    for i,bw_file in enumerate(bw_files):\n",
    "        command = line = f'~/software/bigWigAverageOverBed {bw_file} {outname} avg_sig_{i}.txt'\n",
    "        print('execute --> {}'.format(command))\n",
    "        os.system(command)\n",
    "    dfs = []\n",
    "    for i in range(len(bw_files)):\n",
    "        dfs.append(pd.read_csv(f'avg_sig_{i}.txt',sep='\\t',header=None, index_col=0))\n",
    "    df = pd.DataFrame([dfs[0].iloc[:,0]] + # iloc[:,0]: the width\n",
    "                      [dfi.iloc[:,which_column] for dfi in dfs]).T # conbine the columns together\n",
    "    df.columns = ['width'] + [bw_file.split('/')[-1].split('.')[0] for bw_file in bw_files]\n",
    "    df_bed = pd.read_csv(bedfile, sep='\\t', header=None, names = ['chr', 'start', 'end'])\n",
    "    df_out = df_bed.join(df)\n",
    "    print(f\"output file -->: {bedfile.split('.bed')[0] + '.counts.txt'}\")\n",
    "    df_out.to_csv(bedfile.split('.bed')[0] + '.counts.txt', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. extract the signal\n",
    "merged bed files : out_name = outpath + f'{exp}_{treat}_WT.merge.bed'\n",
    "'./res_avg_signal/293_T1_WT.merge.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bw_files--->:\n",
      "/home/xc3/output/ENL2/ChIP/bw_nsp/293_F_T1_treat.nsp.bw|/home/xc3/output/ENL2/ChIP/bw_nsp/293_F_ENL_treat.nsp.bw|/home/xc3/output/ENL2/ChIP/bw_nsp/293_ctrl.nsp.bw\n",
      "outname--->: ./res_avg_signal/293_T1_WT.merge.test.bed\n",
      "writing the output --> ./res_avg_signal/293_T1_WT.merge.test.bed\n",
      "./res_avg_signal/293_T1_WT.merge.test.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "writing the output --> ./res_avg_signal/293_T1_WT.merge.str.bed\n",
      "./res_avg_signal/293_T1_WT.merge.str.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_F_T1_treat.nsp.bw ./res_avg_signal/293_T1_WT.merge.str.bed avg_sig_0.txt\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_F_ENL_treat.nsp.bw ./res_avg_signal/293_T1_WT.merge.str.bed avg_sig_1.txt\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_ctrl.nsp.bw ./res_avg_signal/293_T1_WT.merge.str.bed avg_sig_2.txt\n",
      "output file -->: ./res_avg_signal/293_T1_WT.merge.counts.txt\n",
      "bw_files--->:\n",
      "/home/xc3/output/ENL2/ChIP/bw_nsp/293_F_T2_treat.nsp.bw|/home/xc3/output/ENL2/ChIP/bw_nsp/293_F_ENL_treat.nsp.bw|/home/xc3/output/ENL2/ChIP/bw_nsp/293_ctrl.nsp.bw\n",
      "outname--->: ./res_avg_signal/293_T2_WT.merge.test.bed\n",
      "writing the output --> ./res_avg_signal/293_T2_WT.merge.test.bed\n",
      "./res_avg_signal/293_T2_WT.merge.test.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "writing the output --> ./res_avg_signal/293_T2_WT.merge.str.bed\n",
      "./res_avg_signal/293_T2_WT.merge.str.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_F_T2_treat.nsp.bw ./res_avg_signal/293_T2_WT.merge.str.bed avg_sig_0.txt\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_F_ENL_treat.nsp.bw ./res_avg_signal/293_T2_WT.merge.str.bed avg_sig_1.txt\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_ctrl.nsp.bw ./res_avg_signal/293_T2_WT.merge.str.bed avg_sig_2.txt\n",
      "output file -->: ./res_avg_signal/293_T2_WT.merge.counts.txt\n",
      "bw_files--->:\n",
      "/home/xc3/output/ENL2/ChIP/bw_nsp/293_F_T3_treat.nsp.bw|/home/xc3/output/ENL2/ChIP/bw_nsp/293_F_ENL_treat.nsp.bw|/home/xc3/output/ENL2/ChIP/bw_nsp/293_ctrl.nsp.bw\n",
      "outname--->: ./res_avg_signal/293_T3_WT.merge.test.bed\n",
      "writing the output --> ./res_avg_signal/293_T3_WT.merge.test.bed\n",
      "./res_avg_signal/293_T3_WT.merge.test.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "writing the output --> ./res_avg_signal/293_T3_WT.merge.str.bed\n",
      "./res_avg_signal/293_T3_WT.merge.str.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_F_T3_treat.nsp.bw ./res_avg_signal/293_T3_WT.merge.str.bed avg_sig_0.txt\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_F_ENL_treat.nsp.bw ./res_avg_signal/293_T3_WT.merge.str.bed avg_sig_1.txt\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_ctrl.nsp.bw ./res_avg_signal/293_T3_WT.merge.str.bed avg_sig_2.txt\n",
      "output file -->: ./res_avg_signal/293_T3_WT.merge.counts.txt\n",
      "bw_files--->:\n",
      "/home/xc3/output/ENL2/ChIP/bw_nsp/293_F_Y78A_treat.nsp.bw|/home/xc3/output/ENL2/ChIP/bw_nsp/293_F_ENL_treat.nsp.bw|/home/xc3/output/ENL2/ChIP/bw_nsp/293_ctrl.nsp.bw\n",
      "outname--->: ./res_avg_signal/293_Y78A_WT.merge.test.bed\n",
      "writing the output --> ./res_avg_signal/293_Y78A_WT.merge.test.bed\n",
      "./res_avg_signal/293_Y78A_WT.merge.test.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "writing the output --> ./res_avg_signal/293_Y78A_WT.merge.str.bed\n",
      "./res_avg_signal/293_Y78A_WT.merge.str.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_F_Y78A_treat.nsp.bw ./res_avg_signal/293_Y78A_WT.merge.str.bed avg_sig_0.txt\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_F_ENL_treat.nsp.bw ./res_avg_signal/293_Y78A_WT.merge.str.bed avg_sig_1.txt\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_ctrl.nsp.bw ./res_avg_signal/293_Y78A_WT.merge.str.bed avg_sig_2.txt\n",
      "output file -->: ./res_avg_signal/293_Y78A_WT.merge.counts.txt\n"
     ]
    }
   ],
   "source": [
    "for treat in treats:\n",
    "    bedfile = outpath + f'{exp}_{treat}_WT.merge.bed'\n",
    "    df_bed = pd.read_csv(bedfile, sep='\\t', header=None, names = ['chr', 'start', 'end'])\n",
    "    bw_files = [glob.glob(bwpath+exp+'*'+tmp+'*.bw')[0] for tmp in [treat,'ENL', 'ctrl']]\n",
    "    print(f'bw_files--->:\\n{\"|\".join(bw_files)}')\n",
    "    outname = bedfile.split('.bed')[0] + '.test.bed'\n",
    "    print(f'outname--->: {outname}')\n",
    "    convert_region_to_bed(bedfile, outname) #format the bedfile as outname, especially for the bigWigAve\n",
    "    comp_avg_sig_from_bw_by_bed(bedfile, bw_files, which_column=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bw_files--->:\n",
      "/home/xc3/output/ENL2/ChIP/bw_nsp/293_F_T1_treat.nsp.bw|/home/xc3/output/ENL2/ChIP/bw_nsp/293_F_ENL_treat.nsp.bw|/home/xc3/output/ENL2/ChIP/bw_nsp/293_ctrl.nsp.bw\n",
      "outname--->: ./res_avg_signal/293_T1_WT.merge.test.bed\n",
      "writing the output --> ./res_avg_signal/293_T1_WT.merge.test.bed\n",
      "./res_avg_signal/293_T1_WT.merge.test.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "writing the output --> ./res_avg_signal/293_T1_WT.merge.str.bed\n",
      "./res_avg_signal/293_T1_WT.merge.str.bed --->: reformated bed_file as the input of the bigWigAverageoverBed\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_F_T1_treat.nsp.bw ./res_avg_signal/293_T1_WT.merge.str.bed avg_sig_0.txt\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_F_ENL_treat.nsp.bw ./res_avg_signal/293_T1_WT.merge.str.bed avg_sig_1.txt\n",
      "execute --> ~/software/bigWigAverageOverBed /home/xc3/output/ENL2/ChIP/bw_nsp/293_ctrl.nsp.bw ./res_avg_signal/293_T1_WT.merge.str.bed avg_sig_2.txt\n",
      "output file -->: ./res_avg_signal/293_T1_WT.merge.counts.txt\n",
      "results --> \n",
      ":     chr   start     end   width  293_F_T1_treat  293_F_ENL_treat  293_ctrl  \\\n",
      "0  chr1  858702  861593  2891.0         59206.0          69010.0   11477.0   \n",
      "1  chr1  870327  871455  1128.0          9481.0          13762.0    3450.0   \n",
      "2  chr1  875392  878614  3222.0         43904.0          53536.0    9646.0   \n",
      "3  chr1  932422  937442  5020.0         91990.0          94608.0   18162.0   \n",
      "4  chr1  955250  956213   963.0          9541.0          11065.0    3152.0   \n",
      "\n",
      "   width_val  \n",
      "0       2891  \n",
      "1       1128  \n",
      "2       3222  \n",
      "3       5020  \n",
      "4        963  \n",
      "3718\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>width</th>\n",
       "      <th>293_F_T1_treat</th>\n",
       "      <th>293_F_ENL_treat</th>\n",
       "      <th>293_ctrl</th>\n",
       "      <th>width_val</th>\n",
       "      <th>validataion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [chr, start, end, width, 293_F_T1_treat, 293_F_ENL_treat, 293_ctrl, width_val, validataion]\n",
       "Index: []"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### tester code\n",
    "exp = '293'\n",
    "bedfile = './res_avg_signal/293_T1_WT.merge.bed'\n",
    "df_bed = pd.read_csv(bedfile, sep='\\t', header=None, names = ['chr', 'start', 'end'])\n",
    "bw_files = [glob.glob(bwpath+exp+'*'+tmp+'*.bw')[0] for tmp in [treat,'ENL', 'ctrl']]\n",
    "print(f'bw_files--->:\\n{\"|\".join(bw_files)}')\n",
    "outname = bedfile.split('.bed')[0] + '.test.bed'\n",
    "print(f'outname--->: {outname}')\n",
    "convert_region_to_bed(bedfile, outname) #format the bedfile as outname, especially for the bigWigAve\n",
    "comp_avg_sig_from_bw_by_bed(bedfile, bw_files)\n",
    "df = pd.read_csv(bedfile.split('.bed')[0] + '.counts.txt', sep='\\t')\n",
    "df['width_val'] = df['end'] - df['start']\n",
    "print(f'results --> \\n: {df.head()}')\n",
    "df['validataion'] = df['width_val'] == df['width'].values.astype(int)\n",
    "print(sum(df['validataion']))\n",
    "df[df['validataion']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Annotate the signal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing on ./res_avg_signal/293_T1_WT.merge.counts.txt\n",
      " will output ./res_avg_signal/293_T1_WT.merge.counts.anno.txt\n",
      "\n",
      "processing on ./res_avg_signal/293_T2_WT.merge.counts.txt\n",
      " will output ./res_avg_signal/293_T2_WT.merge.counts.anno.txt\n",
      "\n",
      "processing on ./res_avg_signal/293_T3_WT.merge.counts.txt\n",
      " will output ./res_avg_signal/293_T3_WT.merge.counts.anno.txt\n",
      "\n",
      "processing on ./res_avg_signal/293_Y78A_WT.merge.counts.txt\n",
      " will output ./res_avg_signal/293_Y78A_WT.merge.counts.anno.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.txt')\n",
    "files_to_annoate\n",
    "infile = files_to_annoate[0]\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.txt')[0] + '.anno.txt'\n",
    "    annoate_bed_to_gene(infile, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate the signal for each peak\n",
    "convert the count files to sig files and store them seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr\tstart\tend\twidth\t293_F_Y78A_treat\t293_F_ENL_treat\t293_ctrl\twithin_genebody\tnearest_TSS\tdistance\r\n",
      "chr1\t858702\t861593\t2891\t42888\t69010\t11477\tSAMD11\tSAMD11,LOC100130417\t473,-3630\r\n"
     ]
    }
   ],
   "source": [
    "!head ./res_avg_signal/293_Y78A_WT.merge.counts.anno.txt -n2\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.txt')\n",
    "files_to_annoate\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.counts.txt')[0] + '.sig.txt'\n",
    "    df = pd.read_csv(infile, header=0, sep='\\t')\n",
    "    df.iloc[:,-3:] = df.iloc[:,-3:].div(df.iloc[:,3],axis=0)\n",
    "    df.to_csv(outfile, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing on ./res_avg_signal/293_T1_WT.merge.sig.txt\n",
      " will output ./res_avg_signal/293_T1_WT.merge.sig.anno.txt\n",
      "\n",
      "processing on ./res_avg_signal/293_T2_WT.merge.sig.txt\n",
      " will output ./res_avg_signal/293_T2_WT.merge.sig.anno.txt\n",
      "\n",
      "processing on ./res_avg_signal/293_T3_WT.merge.sig.txt\n",
      " will output ./res_avg_signal/293_T3_WT.merge.sig.anno.txt\n",
      "\n",
      "processing on ./res_avg_signal/293_Y78A_WT.merge.sig.txt\n",
      " will output ./res_avg_signal/293_Y78A_WT.merge.sig.anno.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## annotate the signal file with the gene annotation\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.sig.txt')\n",
    "files_to_annoate\n",
    "infile = files_to_annoate[0]\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.txt')[0] + '.anno.txt'\n",
    "    annoate_bed_to_gene(infile, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate the counts for edgeR\n",
    "write the count for each paired treat versus ctrl and do the differential analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. generate the files with counts\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.txt')\n",
    "files_to_annoate\n",
    "for infile in files_to_annoate:\n",
    "    outfile_treat = infile.split('.counts.txt')[0] + '.t.txt'\n",
    "    outfile_ctrl = infile.split('.counts.txt')[0] + '.c.txt'\n",
    "    df = pd.read_csv(infile, header=0, sep='\\t')\n",
    "    df.iloc[:,4].to_csv(outfile_treat, sep='\\t',index=True,header=False)\n",
    "    df.iloc[:,5].to_csv(outfile_ctrl, sep='\\t',index=True,header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./res_avg_signal/293_T1_WT.merge.counts.txt', './res_avg_signal/293_T2_WT.merge.counts.txt', './res_avg_signal/293_T3_WT.merge.counts.txt', './res_avg_signal/293_Y78A_WT.merge.counts.txt']\n",
      "./res_avg_signal/293_T1_WT.merge.counts.txt\n",
      "./res_avg_signal/293_T2_WT.merge.counts.txt\n",
      "./res_avg_signal/293_T3_WT.merge.counts.txt\n",
      "./res_avg_signal/293_Y78A_WT.merge.counts.txt\n"
     ]
    }
   ],
   "source": [
    "# 2. write the code for differential analysis\n",
    "# edgeR template should run in the expression folder\n",
    "# run the sh\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.txt')\n",
    "# print(files_to_annoate)\n",
    "with open('de_pval_counts.sh', 'w') as f:\n",
    "    for infile in files_to_annoate:\n",
    "#         print(infile)\n",
    "        outfile_treat = infile.split('/')[-1].split('.counts.txt')[0] + '.t.txt'\n",
    "        outfile_ctrl = infile.split('/')[-1].split('.counts.txt')[0] + '.c.txt'\n",
    "        outfile = infile.split('/')[-1].split('.counts.txt')[0] + '.pval.txt'\n",
    "        line = f'python /home/xc3/software/edgeR.py -s /home/xc3/software/edgeR.template -q 1 -t both {outfile_treat}:1:2 {outfile_ctrl}:1:2 {outfile}'\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. combine the counts and pval together with counts files\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.anno.txt')\n",
    "files_to_annoate\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.txt')[0] + '.pval.txt'\n",
    "    pval_file = infile.split('.counts')[0] + '.pval.txt'\n",
    "    df_pval = pd.read_csv(pval_file, sep='\\t', header=0, index_col='id')\n",
    "    # according to the index to sort the df_pval\n",
    "    df_pval.sort_index(axis=0, inplace=True)\n",
    "    df = pd.read_csv(infile, sep='\\t', header=0)\n",
    "    df_comb = pd.concat([df, df_pval], axis=1)\n",
    "    df_comb.iloc[:, :-3].to_csv(outfile, sep='\\t', index=False)\n",
    "    print(outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./res_avg_signal/293_T1_WT.merge.sig.anno.pval.txt\n",
      "./res_avg_signal/293_T2_WT.merge.sig.anno.pval.txt\n",
      "./res_avg_signal/293_T3_WT.merge.sig.anno.pval.txt\n",
      "./res_avg_signal/293_Y78A_WT.merge.sig.anno.pval.txt\n"
     ]
    }
   ],
   "source": [
    "# 3. combine the sig and pval together with counts files\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.sig.anno.txt')\n",
    "files_to_annoate\n",
    "for infile in files_to_annoate:\n",
    "    outfile = infile.split('.txt')[0] + '.pval.txt'\n",
    "    pval_file = infile.split('.sig')[0] + '.pval.txt'\n",
    "    df_pval = pd.read_csv(pval_file, sep='\\t', header=0, index_col='id')\n",
    "    # according to the index to sort the df_pval\n",
    "    df_pval.sort_index(axis=0, inplace=True)\n",
    "    df = pd.read_csv(infile, sep='\\t', header=0)\n",
    "    df_comb = pd.concat([df, df_pval], axis=1)\n",
    "    df_comb.iloc[:, :-3].to_csv(outfile, sep='\\t', index=False)\n",
    "    print(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./res_avg_signal/293_T1_WT.merge.counts.anno.genes1_2.txt\n",
      "./res_avg_signal/293_T1_WT.merge.counts.anno.genes1_5.txt\n",
      "./res_avg_signal/293_T2_WT.merge.counts.anno.genes1_2.txt\n",
      "./res_avg_signal/293_T2_WT.merge.counts.anno.genes1_5.txt\n",
      "./res_avg_signal/293_T3_WT.merge.counts.anno.genes1_2.txt\n",
      "./res_avg_signal/293_T3_WT.merge.counts.anno.genes1_5.txt\n",
      "./res_avg_signal/293_Y78A_WT.merge.counts.anno.genes1_2.txt\n",
      "./res_avg_signal/293_Y78A_WT.merge.counts.anno.genes1_5.txt\n"
     ]
    }
   ],
   "source": [
    "# 4. extracted the gene lists with fold change for 1.2 and 1.5\n",
    "files_to_annoate = glob.glob(f'{outpath}*merge.counts.anno.txt')\n",
    "files_to_annoate\n",
    "for infile in files_to_annoate:\n",
    "    outfile1 = infile.split('.txt')[0] + '.genes1_2.txt'\n",
    "    outfile2 = infile.split('.txt')[0] + '.genes1_5.txt'\n",
    "    df = pd.read_csv(infile, sep='\\t', header=0)\n",
    "    df['fc'] = df.iloc[:,4].div(df.iloc[:,5])\n",
    "    # combine the two dataframes into the list\n",
    "    dfs_list = [df[df['fc'] >= 1.2], df[df['fc'] >= 1.5]]\n",
    "    # combine the outfiles names into the list \n",
    "    outfiles = [outfile1, outfile2]\n",
    "    # extract the genes\n",
    "    for tmp, outfile in zip(dfs_list, outfiles):\n",
    "        body_tss = []\n",
    "        body = []\n",
    "        tss = []\n",
    "        for cols in tmp.iloc[:,7]:\n",
    "            eles = cols.split(',')\n",
    "            for ele in eles:\n",
    "                if ele and ele != 'none':\n",
    "                    body.append(ele)\n",
    "\n",
    "        for cols in tmp.iloc[:,8]:\n",
    "            eles = cols.split(',')\n",
    "            for ele in eles:\n",
    "                if ele and ele != 'none':\n",
    "                    tss.append(ele)\n",
    "\n",
    "        body.extend(tss)\n",
    "        body = list(set(body))\n",
    "        with open(outfile, 'w') as f:\n",
    "            f.writelines('\\n'.join(body))\n",
    "            \n",
    "        print(outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_annoate = glob.glob(f'{outpath}*.counts.anno.txt')\n",
    "files_to_annoate\n",
    "infile = files_to_annoate[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3761, 11)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(infile, sep='\\t', header=0)\n",
    "df['fc'] = df.iloc[:,4].div(df.iloc[:,5])\n",
    "dfs_list = [df[df['fc'] >= 1.2], df[df['fc'] >= 1.5]]\n",
    "dfs_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
